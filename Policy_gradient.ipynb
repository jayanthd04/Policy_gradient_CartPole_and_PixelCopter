{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d2025d6-7fbe-43b5-bf7b-23cc8f66c880",
   "metadata": {},
   "source": [
    "## Policy-based models\n",
    "\n",
    "In the past, we have learned about value-based models such as Q-learning, where we use a Q-table to figure out the value of being in a state and taking an action, or deep q-learning where we use a neural-net to estimate the Q-value of a every state and action pairs. \n",
    "\n",
    "In value-based methodologies, the optimal policy is simply the action that results in the highest Q-value for each state. Unlike, value-based models, in policy-based models, we train the policy directly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489abead-8104-4139-b763-99b5b5f790ca",
   "metadata": {},
   "source": [
    "## What are the policy-based methods? \n",
    "\n",
    "The main goal of Reinforcement learning is for our agent to learn the optimial policy that will maximize the expected cumulative reward. RL is based on the reward hypothesis which states that all goals can be described as the maximization of the expected cumulative reward. \n",
    "\n",
    "Unlike how in value-based methods our agent learns a value function which implicitly contains the optimal policy, in policy-based methods, we directly learn a optimal policy without the use of a value function. \n",
    "* First we parameterize policy. We use a policy net that will output the prob distribution of all actions.\n",
    "* $pi(s) = P[A|s; theta]$\n",
    "* Our goal would then be to maximize the performance(reward gained) of the policy using gradient ascent to update the parameters of the policy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ddf7a-123c-4927-9d90-277fb94d4466",
   "metadata": {},
   "source": [
    "## The difference between policy-based and policy-gradient methods\n",
    "\n",
    "Policy-gradient methods are a subset of policy-based methods. In policy-based methods, optimization is on-policy since we use the data from our most recent policy to update our parameters. \n",
    "\n",
    "The difference between policy-based and policy-gradient methods is based on how we optimize the parameters:\n",
    "\n",
    "* In policy-based, we optimize the parameters indirectly by maximizing the local approximation of the objective function with tech such as hill climbing, simulated annealing, or evolution strategies.\n",
    "* In policy-gradient, we optimize the parameters directly by using gradient ascent on the performance of the objective function $J(theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1af5a4-0e5d-4769-b199-48f5b90628f8",
   "metadata": {},
   "source": [
    "## Advantages and disadvantages of policy-gradient methods\n",
    "\n",
    "### Advantages\n",
    "\n",
    "#### The simplicity of integration \n",
    "There is no memory overhead from the action value storage. We estimate the policy directly. \n",
    "\n",
    "#### Policy-gradient methods can learn stochastic policies \n",
    "\n",
    "Policy gradients can learn stochastic policies while value functions virtually can't \n",
    "\n",
    "There are two consequences that arise from this: \n",
    "\n",
    "1. We don't need to implement exploration/exploitation trade-off. Since our model outputs a probability distribution over actions, the agent can explore the env without taking the same trajectory all the time.\n",
    "2. We also eliminate the problem of perpetual aliasing. Perpetual aliasing is when two states are perceived as the same for our model. Our agent can get stuck in aliased states and never reach the goal, or spends a lot of time not reaching the goal.\n",
    "\n",
    "In a stochastic policy, actions are chosen more randomly thus it will not get stuck and have a higher probability of reaching the goal. \n",
    "\n",
    "Policy-gradient methods are better in high-dimensional action-spaces and continuous action spaces. \n",
    "The main problem with Deep Q-learning is that they assign a value for each possible action which is great for discrete action spaces but terrible for continous or high-dimensional action spaces. Policy-gradients fix this problem by returning a probability distribution over all actions. \n",
    "\n",
    "#### Policy-gradient methods are better at convergence \n",
    "In value-based methods, we change the policy more aggressively as we take the max Q-value for even small gains. In policy-gradient methods on the other hand, we change the policy more smoothly.\n",
    "\n",
    "### Disadvantages \n",
    "* Policy-gradient methods have a tendency to converget at a local maximum instead of global maximum.\n",
    "* Training Policy-gradients takes a lot of time to train.\n",
    "* Policy-gradient tends to have high variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c0e274-bb28-4f54-9ea3-e754b8e6da47",
   "metadata": {},
   "source": [
    "## Policy-gradient methods\n",
    "\n",
    "### Big picture\n",
    "We know that in policy-gradient methods, our goal is to find parameters that maximize the expected return. \n",
    "\n",
    "The idea behind policy-gradient methods is that we have a parameterized stochastic policy. For our case, the policy is a neural net which outputs a probability distribution over all actions. The probability of taking each action is called an action preference. \n",
    "\n",
    "Our goal when training using policy-gradient methods is that we want to adjust the probability distribution of all actions such that those that are optimal have a higher probability of being sampled. We tweak the parameters each time agent interacts with the env. \n",
    "\n",
    "**How do we optimize the weights of our neural net?**\n",
    "\n",
    "We let our agent interact with the env for an episode and accumulate rewards. If we get a positive reward, we use increase the weights accordingly and decrease weights if we receive a negative reward. \n",
    "\n",
    "**How do we know if our policy is good?** We use a score/objective function called $J(theta)$\n",
    "\n",
    "#### The objective function\n",
    "\n",
    "The objective function gives us the performance of our agent given a trajectory(state action sequence without the rewared), and outputs the expected cumulative reward. \n",
    "\n",
    "$J(theta) = E_{tau ~ pi}[R(tau)]$ \n",
    "$R(tau) = r_{t+1} + gamma*r_{t+2}+ gamma^2*r_{t+3}+......$\n",
    "(tau) = trajectory\n",
    "* The expected return is the weighted average of all possible values that return R(tau) can return.\n",
    "* $J(theta) = summation(P(tau;theta)R(tau))$\n",
    "    - R(tau): return from an arbitrary trajectory.\n",
    "    - P(tau;theta): probability of each possible trajectory tau.\n",
    "    - J(theta): Expected return, which is calculated by summing all possible trajectories, the probabilities of taking each trajectory given theta multiplied by the return gained from this trajectory.\n",
    "#### Gradient ascent and the policy-gradient theorem \n",
    "Gradient ascent is simply the inverse of gradient descent since we want to maximize the reward. We update our parameters based on this formula: \n",
    "\n",
    "$theta <--- theta+ lr*J(theta)'$\n",
    "\n",
    "There are two problems we encounter when we calculate the derivative of $J(theta)$\n",
    "1. We can't really calculate the derivative of our objective function since we need to calculate the probability of each possible trajectory, which can be really expensive. Instead, we calculate an estimate of our gradient using some sample trajectories we collect.\n",
    "2. In order to differentiate the objective function, we need to differentiate the markov decision process dynamics, which is tied to the env. We most likely won't be knowing this as such we can't differentiate it.\n",
    "\n",
    "To address the above problems, we will use the Policy Gradient Theorem which reformulates the objective function into a differentiable function. \n",
    "\n",
    "**The Policy Gradient Theorem**\n",
    "\n",
    "$J(theta)' = E_{pi_{theta}}[log pi_{theta}(a_t| s_t)'R(tau)']$\n",
    "\n",
    "### The Monte Carlo Reinforce \n",
    "The Monte-carlo policy gradient, is a policy-gradient that the estimated return from an entire episode to update the policy parameters. \n",
    "\n",
    "In the loop: \n",
    "* Use the policy pi to collect episodes\n",
    "* Use the episode to estimate the gradient $g = J(theta)'$\n",
    "    * $J(theta)' = summation((log pi_{theta}(a_t|s_t)*R(tau))')$\n",
    "* Update the weights of the policy using $theta <--- theta+ lr*g$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08b6222-6bec-4fea-81a7-ba055689e5ba",
   "metadata": {},
   "source": [
    "## Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea6417cf-6e24-4b08-9359-ac4a0920975c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "from collections import deque \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F \n",
    "import torch.optim as optim \n",
    "from torch.distributions import Categorical \n",
    "\n",
    "import gym \n",
    "import gym_pygame\n",
    "import ple.games.pixelcopter\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0b0ea93-dc7a-48ff-a97d-68b80a45ddce",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502589d6-af1a-4291-a4eb-83dbaa8e36b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed802e8-1944-405e-b284-7b7ce8337192",
   "metadata": {},
   "source": [
    "## CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc27acdd-a6bc-4c5a-abd5-c0e7836310f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"CartPole-v1\"\n",
    "\n",
    "env = gym.make(env_id)\n",
    "\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38c36e24-e58b-4a32-b703-95b0f4fe84fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Observation Space_______\n",
      "The state space is:  4\n",
      "Sample Observation [-3.9728885e+00 -2.1653244e+36  3.6817831e-01  1.9501910e+38]\n"
     ]
    }
   ],
   "source": [
    "print(\"____Observation Space_______\")\n",
    "print(\"The state space is: \", state_size)\n",
    "print(\"Sample Observation\", env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3905c71f-c686-48e9-831b-1b97b23e4cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____Action Space_______\n",
      "The action space is:  2\n",
      "Action Space Sample 1\n"
     ]
    }
   ],
   "source": [
    "print(\"____Action Space_______\")\n",
    "print(\"The action space is: \", action_size)\n",
    "print(\"Action Space Sample\", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8155ae7-85f9-4992-b616-536f8a0570a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self,state_size,action_size,hidden_size):\n",
    "        super(Policy,self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size,action_size)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x,dim=1)\n",
    "    def act(self,state):\n",
    "        \"\"\"\n",
    "        Given a state, take action\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f8042c2-34b9-4012-b392-e46433dd1712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, tensor([-0.5260], grad_fn=<SqueezeBackward1>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_pol = Policy(state_size,action_size,64).to(device)\n",
    "state,info = env.reset()\n",
    "debug_pol.act(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e93fb8-b36d-4d84-ac4d-6ae2a6bc4b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy,optimizer, n_training_episodes, max_t, gamma, print_every): \n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "\n",
    "    for i_episode in tqdm(range(1,n_training_episodes+1)):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(max_t):\n",
    "            action,log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state,reward,done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done: \n",
    "                break\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "\n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft(gamma* disc_return_t+rewards[t])\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "\n",
    "        policy_loss = []\n",
    "        for log_prob, disc_return in zip(saved_log_probs,returns):\n",
    "            policy_loss.append(-log_prob * disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every ==0:\n",
    "            print(\"Episode {}\\tAverage Score: {:.2f}\".format(i_episode, np.mean(scores_deque)))\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42bd4287-4343-4b10-b887-18e306e62bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_hyperparams = {\n",
    "    \"h_size\":16,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\":1.0,\n",
    "    \"lr\":1e-2,\n",
    "    \"env_id\":env_id,\n",
    "    \"state_space\":state_size,\n",
    "    \"action_space\":action_size,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02167f80-de70-4cc8-b3b6-8a53632b3368",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_policy = Policy(\n",
    "    cartpole_hyperparams[\"state_space\"],\n",
    "    cartpole_hyperparams[\"action_space\"],\n",
    "    cartpole_hyperparams[\"h_size\"],\n",
    ").to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr = cartpole_hyperparams[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad9b9c4c-5b63-4430-a24e-6cfdb5b78e63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51fc36064b7c49659568cced06bdf665",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 25.72\n",
      "Episode 200\tAverage Score: 76.19\n",
      "Episode 300\tAverage Score: 729.94\n",
      "Episode 400\tAverage Score: 985.55\n",
      "Episode 500\tAverage Score: 309.37\n",
      "Episode 600\tAverage Score: 616.85\n",
      "Episode 700\tAverage Score: 320.12\n",
      "Episode 800\tAverage Score: 990.03\n",
      "Episode 900\tAverage Score: 1000.00\n",
      "Episode 1000\tAverage Score: 1000.00\n"
     ]
    }
   ],
   "source": [
    "scores = reinforce(\n",
    "    cartpole_policy,\n",
    "    cartpole_optimizer,\n",
    "    cartpole_hyperparams[\"n_training_episodes\"],\n",
    "    cartpole_hyperparams[\"max_t\"],\n",
    "    cartpole_hyperparams[\"gamma\"],\n",
    "    100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8578ca2b-2c35-420d-9658-58740264e051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 16.0\n"
     ]
    }
   ],
   "source": [
    "disp_env = gym.make('CartPole-v1',render_mode=\"human\")\n",
    "episodes=10\n",
    "observation,info=disp_env.reset()\n",
    "done=False\n",
    "score=0\n",
    "steps=0;\n",
    "while not done :\n",
    "    with torch.no_grad():\n",
    "        action, _ = cartpole_policy.act(observation)\n",
    "    observation,reward,done,truncated,info=disp_env.step(action)\n",
    "    score+=reward\n",
    "    steps+=1;\n",
    "    done = done or truncated\n",
    "    disp_env.render()\n",
    "print(f\"Score: {score}\")\n",
    "disp_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c19a1c60-7323-4f83-9305-9e21ec50b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "    \"\"\"\n",
    "    Evaluate the agent for ``n_eval_episodes`` episodes and return the average reward and std of reward.\n",
    "    :param env: The evaluation environment \n",
    "    :param n_eval_episodes: Number of episodes to evaluate the agent\n",
    "    :param policy: The Reinforce agent\n",
    "    \"\"\"\n",
    "    episode_rewards=[]\n",
    "    for episode in tqdm(range(n_eval_episodes)):\n",
    "        state,_ = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep=0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            with torch.no_grad():\n",
    "                action,_ = policy.act(state)\n",
    "            new_state, reward,done,truncated, info = env.step(action)\n",
    "            total_rewards_ep+=reward\n",
    "            if done: \n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f8a7afc-d0e8-4c22-b24e-64c391797bec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb986c2c52674fd495a8848ad0cc6de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1000.0, 0.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_agent(\n",
    "    eval_env, cartpole_hyperparams[\"max_t\"], cartpole_hyperparams[\"n_evaluation_episodes\"], cartpole_policy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3eecc6-e316-4945-b6b5-c83551fd6d4c",
   "metadata": {},
   "source": [
    "## PixelCopter Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bc21bb8-dc8b-4cec-af68-4389f92f0bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = \"Pixelcopter-PLE-v0\"\n",
    "env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae5bc8f0-af32-4ca9-a4f9-7730bd55b5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_______\n",
      "The State Space is:  7\n",
      "State Space Sample:  [ 1.4560156   0.71981394 -0.11523677 -1.836842    0.14641193  0.39520997\n",
      " -1.4625291 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"_____OBSERVATION SPACE_______\")\n",
    "print(\"The State Space is: \", state_size)\n",
    "print(\"State Space Sample: \", env.observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26e47f8f-7653-4813-9d4b-4512b10f42af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____ACTION SPACE_______\n",
      "The Action Space is:  2\n",
      "Action Space Sample:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"_____ACTION SPACE_______\")\n",
    "print(\"The Action Space is: \", action_size)\n",
    "print(\"Action Space Sample: \", env.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "966a7828-8ebd-49d3-b8e0-4ef4dac5b198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy2(nn.Module):\n",
    "    def __init__(self,state_size, action_size, hidden_size):\n",
    "        super(Policy2,self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size,hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size*2)\n",
    "        self.fc3 = nn.Linear(hidden_size*2, action_size)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x,dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26d509af-56f3-44e8-a7dc-a5342b638660",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelcopter_hyperparams = {\n",
    "    \"hidden_size\" : 64,\n",
    "    \"n_training_episodes\": 50000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 10000,\n",
    "    \"gamma\":0.99,\n",
    "    \"lr\": 1e-4,\n",
    "    \"env_id\": env_id,\n",
    "    \"state_size\": state_size,\n",
    "    \"action_size\": action_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb2efdeb-3f7c-4ea5-99a0-ff94f187b47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixelcopter_policy = Policy2(\n",
    "    pixelcopter_hyperparams[\"state_size\"],\n",
    "    pixelcopter_hyperparams[\"action_size\"],\n",
    "    pixelcopter_hyperparams[\"hidden_size\"]\n",
    ").to(device)\n",
    "pixelcopter_optim = optim.Adam(pixelcopter_policy.parameters(),lr = pixelcopter_hyperparams[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9fbb9d7-4265-46cd-9c40-e5b838fa56bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe59228815fc46ef8856ec1236c4fe58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000\tAverage Score: 6.09\n",
      "Episode 2000\tAverage Score: 5.92\n",
      "Episode 3000\tAverage Score: 5.89\n",
      "Episode 4000\tAverage Score: 8.83\n",
      "Episode 5000\tAverage Score: 12.85\n",
      "Episode 6000\tAverage Score: 13.92\n",
      "Episode 7000\tAverage Score: 17.04\n",
      "Episode 8000\tAverage Score: 17.83\n",
      "Episode 9000\tAverage Score: 14.05\n",
      "Episode 10000\tAverage Score: 18.26\n",
      "Episode 11000\tAverage Score: 20.32\n",
      "Episode 12000\tAverage Score: 22.01\n",
      "Episode 13000\tAverage Score: 15.12\n",
      "Episode 14000\tAverage Score: 23.52\n",
      "Episode 15000\tAverage Score: 19.70\n",
      "Episode 16000\tAverage Score: 26.63\n",
      "Episode 17000\tAverage Score: 22.63\n",
      "Episode 18000\tAverage Score: 13.52\n",
      "Episode 19000\tAverage Score: 25.25\n",
      "Episode 20000\tAverage Score: 26.05\n",
      "Episode 21000\tAverage Score: 25.81\n",
      "Episode 22000\tAverage Score: 31.77\n",
      "Episode 23000\tAverage Score: 33.98\n",
      "Episode 24000\tAverage Score: 25.72\n",
      "Episode 25000\tAverage Score: 32.08\n",
      "Episode 26000\tAverage Score: 35.19\n",
      "Episode 27000\tAverage Score: 36.19\n",
      "Episode 28000\tAverage Score: 33.71\n",
      "Episode 29000\tAverage Score: 25.45\n",
      "Episode 30000\tAverage Score: 34.90\n",
      "Episode 31000\tAverage Score: 46.48\n",
      "Episode 32000\tAverage Score: 43.54\n",
      "Episode 33000\tAverage Score: 37.65\n",
      "Episode 34000\tAverage Score: 52.04\n",
      "Episode 35000\tAverage Score: 42.86\n",
      "Episode 36000\tAverage Score: 24.99\n",
      "Episode 37000\tAverage Score: 50.03\n",
      "Episode 38000\tAverage Score: 33.37\n",
      "Episode 39000\tAverage Score: 60.35\n",
      "Episode 40000\tAverage Score: 48.61\n",
      "Episode 41000\tAverage Score: 52.10\n",
      "Episode 42000\tAverage Score: 46.07\n",
      "Episode 43000\tAverage Score: 56.61\n",
      "Episode 44000\tAverage Score: 50.40\n",
      "Episode 45000\tAverage Score: 62.80\n",
      "Episode 46000\tAverage Score: 73.99\n",
      "Episode 47000\tAverage Score: 58.79\n",
      "Episode 48000\tAverage Score: 69.69\n",
      "Episode 49000\tAverage Score: 39.56\n",
      "Episode 50000\tAverage Score: 68.14\n"
     ]
    }
   ],
   "source": [
    "scores = reinforce(\n",
    "    pixelcopter_policy,\n",
    "    pixelcopter_optim,\n",
    "    pixelcopter_hyperparams[\"n_training_episodes\"],\n",
    "    pixelcopter_hyperparams[\"max_t\"],\n",
    "    pixelcopter_hyperparams[\"gamma\"],\n",
    "    1000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1622211d-26c6-4e15-ae15-4d87adbb8942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model already saved\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path \n",
    "model_path = Path()\n",
    "model_dir = model_path / \"Pixelcopter.pth\"\n",
    "if model_dir.exists():\n",
    "    print(\"Best model already saved\")\n",
    "else:\n",
    "    print(f\"Saving model to: {model_dir}\")\n",
    "    torch.save(obj = pixelcopter_policy.state_dict(),\n",
    "          f = model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0b2dba-32ee-452c-82ed-ee5d883bc8a7",
   "metadata": {},
   "source": [
    "## Loading model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "04b69377-c4eb-48f7-80df-fb6c6f165927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixelcopter_policy.load_state_dict(torch.load(f=model_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "473fbe38-22b9-4d5c-85f4-b9005b8f22ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'pyglet_rendering' from 'gym.utils' (C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m     score\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mreward\n\u001b[0;32m     12\u001b[0m     steps\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m;\n\u001b[1;32m---> 13\u001b[0m     \u001b[43mdisp_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m disp_env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\wrappers\\order_enforcing.py:51\u001b[0m, in \u001b[0;36mOrderEnforcing.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_disable_render_order_enforcing \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\n\u001b[0;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call `env.render()` before calling `env.reset()`, if this is a intended action, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     49\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset `disable_render_order_enforcing=True` on the OrderEnforcer wrapper.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m     )\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\wrappers\\env_checker.py:53\u001b[0m, in \u001b[0;36mPassiveEnvChecker.render\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchecked_render \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43menv_render_passive_checker\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mrender(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:316\u001b[0m, in \u001b[0;36menv_render_passive_checker\u001b[1;34m(env, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m env\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01min\u001b[39;00m render_modes, (\n\u001b[0;32m    312\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe environment was initialized successfully however with an unsupported render mode. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    313\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRender mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mrender_mode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, modes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrender_modes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    314\u001b[0m         )\n\u001b[1;32m--> 316\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;66;03m# TODO: Check that the result is correct\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym_pygame\\envs\\base.py:64\u001b[0m, in \u001b[0;36mBaseEnv.render\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyglet_rendering \u001b[38;5;28;01mas\u001b[39;00m rendering\n\u001b[0;32m     65\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mviewer \u001b[38;5;241m=\u001b[39m rendering\u001b[38;5;241m.\u001b[39mSimpleImageViewer()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'pyglet_rendering' from 'gym.utils' (C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\__init__.py)"
     ]
    }
   ],
   "source": [
    "disp_env = gym.make('Pixelcopter-PLE-v0')\n",
    "episodes=10\n",
    "observation=disp_env.reset()\n",
    "done=False\n",
    "score=0\n",
    "steps=0;\n",
    "while not done :\n",
    "    with torch.no_grad():\n",
    "        action, _ = pixelcopter_policy.act(observation)\n",
    "    observation,reward,done,info=disp_env.step(action)\n",
    "    score+=reward\n",
    "    steps+=1;\n",
    "    disp_env.render('human')\n",
    "print(f\"Score: {score}\")\n",
    "disp_env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "779479cd-02ef-4bde-9ce1-beba615b8f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymgrid2\n",
      "  Downloading gymgrid2-1.2.6-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: gym in c:\\users\\jayan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymgrid2) (0.26.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\jayan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gymgrid2) (1.25.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\jayan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gym->gymgrid2) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in c:\\users\\jayan\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gym->gymgrid2) (0.0.8)\n",
      "Downloading gymgrid2-1.2.6-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: gymgrid2\n",
      "Successfully installed gymgrid2-1.2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install gymgrid2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df6a62de-64a7-4c01-beb9-07eea491e476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent2(env, max_steps, n_eval_episodes, policy):\n",
    "    \"\"\"\n",
    "    Evaluate the agent for ``n_eval_episodes`` episodes and return the average reward and std of reward.\n",
    "    :param env: The evaluation environment \n",
    "    :param n_eval_episodes: Number of episodes to evaluate the agent\n",
    "    :param policy: The Reinforce agent\n",
    "    \"\"\"\n",
    "    episode_rewards=[]\n",
    "    for episode in tqdm(range(n_eval_episodes)):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep=0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            with torch.no_grad():\n",
    "                action,_ = policy.act(state)\n",
    "            new_state, reward,done, info = env.step(action)\n",
    "            total_rewards_ep+=reward\n",
    "            if done: \n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3932a81e-ac48-43d2-9c21-3bdb3870c815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f116443b0e4258bb7e09c2812e52b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:225: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(done, (bool, np.bool8)):\n",
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\jayan\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(43.0, 57.37246726435948)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_agent2(eval_env, pixelcopter_hyperparams[\"max_t\"], pixelcopter_hyperparams[\"n_evaluation_episodes\"],pixelcopter_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efdd257-7af4-4a5a-9b6f-bef5dbf3ded6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
